# Sprint 0: Infrastructure Setup - Quick-Start Checklist

**Project:** Mail Reactor  
**Date Created:** 2025-11-26  
**Date Updated:** 2025-11-27  
**Owner:** Tech Lead + HC  
**Estimated Effort:** 26 hours (~3-3.5 days)  
**Actual Effort (Task #1):** ~4 hours (minimal configuration approach)  
**Status:** In Progress - Task #1 Complete ‚úÖ

## Progress Summary

- ‚úÖ **Task #1:** Environment Setup & Verification (Complete - 2025-11-27)
- ‚è∏Ô∏è **Task #2:** TDD Infrastructure (Pending TEA input)
- ‚è≥ **Task #3:** Mock IMAP/SMTP Server Setup
- ‚è≥ **Task #4:** Test Project Structure
- ‚è≥ **Task #5:** Security Scanning Setup
- ‚è≥ **Task #6:** Performance Benchmark Infrastructure
- ‚è≥ **Task #7:** CI/CD Pipeline Configuration

**Key Achievements:**
- Created minimal, cross-platform environment setup
- Documented "add only when needed" configuration principle
- Automated verification script works on all platforms
- Windows setup verified and working

---

## ‚ö†Ô∏è CRITICAL: Task Sequencing

**BLOCKING TASK:** Task #1 (Environment Setup) MUST complete before all other tasks.

**Recommended Sequence:**
1. **Day 1-2:** Task #1 (Environment Setup & Verification) - **BLOCKING**
2. **Day 2:** TDD Training Workshop (after Task #1 verified)
3. **Day 3-4:** Tasks #2-7 (can run in parallel after Task #1)

**Gate Check:** All verification checklists pass before proceeding to Task #2-7.

---

## Task #1: Environment Setup & Verification (BLOCKING) ‚úÖ COMPLETE

**Priority:** üî¥ **CRITICAL - Must complete first**  
**Estimated Effort:** 10 hours (~1-1.5 days)  
**Actual Effort:** ~4 hours (minimal configuration approach)  
**Owner:** Tech Lead + HC  
**Status:** ‚úÖ Complete - 2025-11-27

### Subtask 1.1: Create Development Environment Configurations (3h) ‚úÖ

**Deliverables:**
- [x] `flake.nix` created in project root (minimal: Python 3.10 + uv only)
- [x] `.envrc` created in project root (minimal: nix-direnv + use flake)
- [x] `pyproject.toml` created in project root (minimal: basic dev deps)
- [x] `.gitignore` updated (added .direnv/)
- [x] `src/mailreactor/__init__.py` created (package structure)
- [x] Configuration principles documented in development-practices.md

**Notes:** Applied "add only when needed" principle - removed docker, git, unused dependencies

**Content Requirements:**

**`flake.nix` (Nix users - macOS, Linux, WSL2):**
```nix
{
  description = "Mail Reactor - Headless Email Client";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs = { self, nixpkgs, flake-utils }:
    flake-utils.lib.eachDefaultSystem (system:
      let
        pkgs = nixpkgs.legacyPackages.${system};
      in
      {
        devShells.default = pkgs.mkShell {
          buildInputs = with pkgs; [
            python310
            uv
            git
            docker
            docker-compose
          ];

          shellHook = ''
            echo "Mail Reactor Development Environment"
            echo "Python: $(python --version)"
            echo "uv: $(uv --version)"
            echo ""
            echo "Quick start:"
            echo "  uv venv"
            echo "  source .venv/bin/activate"
            echo "  uv pip install -e \".[dev]\""
          '';
        };
      }
    );
}
```

**`.envrc` (direnv auto-activation):**
```bash
use flake
```

**`pyproject.toml` (Project metadata):**
```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "mailreactor"
version = "0.1.0"
description = "Headless email client with REST API"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "HC", email = "your-email@example.com"},
]
dependencies = [
    "fastapi>=0.122.0",
    "uvicorn[standard]",
    "imapclient>=3.0.1",
    "aiosmtplib>=5.0.0",
    "typer>=0.20.0",
    "pydantic>=2.0",
    "pydantic-settings>=2.0",
    "structlog",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-asyncio",
    "pytest-cov",
    "pytest-benchmark",
    "httpx",
    "ruff",
    "mypy",
    "pre-commit",
    "detect-secrets",
]

[project.scripts]
mailreactor = "mailreactor.cli:app"

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.mypy]
python_version = "3.10"
strict = true

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
```

**`.gitignore` additions:**
```gitignore
# Python
.venv/
__pycache__/
*.pyc
*.pyo
*.egg-info/
dist/
build/

# Nix
.direnv/
result

# IDE
.vscode/
.idea/
*.swp

# Environment
.env
.env.local

# Test artifacts
.coverage
.pytest_cache/
htmlcov/

# Secrets
/secrets/
*.key
```

**Checklist:**
- [x] All files created
- [x] Files committed to git
- [x] `.gitignore` prevents sensitive files from being committed

---

### Subtask 1.2: Write Environment Setup Documentation (2h) ‚úÖ

**Deliverables:**
- [x] `docs/environment-setup-guide.md` created (minimal: just prerequisites + setup steps)
- [x] `README.md` updated with honest, minimal description
- [x] Points to official installation sources (Nix, direnv)

**Notes:** Kept documentation minimal - relies on flake shellHook for detailed instructions

**`docs/environment-setup-guide.md` Structure:**

```markdown
# Environment Setup Guide

## Overview

Mail Reactor supports two environment setup methods:
1. **Nix Flakes + direnv** (macOS, Linux, WSL2) - Recommended
2. **Manual Setup** (Windows, Linux without Nix)

Both methods produce identical Python environments.

---

## Method 1: Nix Flakes + direnv (macOS, Linux, WSL2)

### Prerequisites
- Nix package manager installed (https://nixos.org/download.html)
- direnv installed (https://direnv.net/docs/installation.html)

### Setup Steps
1. Clone repository
2. Allow direnv: `direnv allow`
3. Create virtual environment: `uv venv`
4. Activate: `source .venv/bin/activate`
5. Install dependencies: `uv pip install -e ".[dev]"`

### Verification Checklist
- [ ] `python --version` shows Python 3.10+
- [ ] `uv --version` works
- [ ] `pytest --version` works
- [ ] `import mailreactor` succeeds (after install)

---

## Method 2: Manual Setup (Windows, Linux without Nix)

### Prerequisites
- Python 3.10+ installed
- pip installed

### Setup Steps
1. Clone repository
2. Install uv: `pip install uv`
3. Create virtual environment: `uv venv`
4. Activate:
   - Windows: `.venv\Scripts\activate`
   - Linux: `source .venv/bin/activate`
5. Install dependencies: `uv pip install -e ".[dev]"`

### Verification Checklist
- [ ] `python --version` shows Python 3.10+
- [ ] `uv --version` works
- [ ] `pytest --version` works
- [ ] `import mailreactor` succeeds (after install)

---

## Troubleshooting

### Issue: direnv not auto-activating
**Solution:** Run `direnv allow` in project root

### Issue: Python version mismatch
**Nix:** Update `python310` in `flake.nix`
**Manual:** Install correct Python version

### Issue: uv not found
**Nix:** Nix shell includes uv automatically
**Manual:** `pip install uv`

### Issue: Import mailreactor fails
**Solution:** Run `uv pip install -e ".[dev]"` first
```

**Checklist:**
- [x] Documentation complete with both methods
- [x] Points to official installation sources
- [x] Verification script included
- [x] README.md updated with link to guide

---

### Subtask 1.3: Verify Nix Setup (macOS + Linux/WSL2) (2h) ‚úÖ

**Deliverables:**
- [x] Created `verify-setup.sh` Python script (cross-platform)
- [x] Automated verification of Python 3.10+, uv, venv, dev tools
- [x] Clear error messages with actionable instructions
- [x] Integrated into flake shellHook quickstart

**Platforms:**
- [x] Cross-platform script (works on macOS, Linux, WSL2, Windows)

**Verification Steps (per platform):**

1. **Environment Activation:**
   ```bash
   cd /path/to/mailreactor
   direnv allow
   # Should see: "direnv: loading ~/mailreactor/.envrc"
   ```
   - [ ] direnv activates automatically

2. **Python Version:**
   ```bash
   python --version
   # Expected: Python 3.10.x or higher
   ```
   - [ ] Python 3.10+ confirmed

3. **uv Package Manager:**
   ```bash
   uv --version
   # Expected: uv x.x.x
   ```
   - [ ] uv installed and working

4. **Virtual Environment:**
   ```bash
   uv venv
   source .venv/bin/activate
   ```
   - [ ] .venv created successfully
   - [ ] Virtual environment activated

5. **Dependency Installation:**
   ```bash
   uv pip install -e ".[dev]"
   # Should install FastAPI, pytest, etc.
   ```
   - [ ] All dependencies installed without errors

6. **Import Test:**
   ```bash
   python -c "import sys; print(sys.version)"
   # Expected: 3.10.x or higher
   ```
   - [ ] Python import works

7. **Pytest Available:**
   ```bash
   pytest --version
   # Expected: pytest x.x.x
   ```
   - [ ] pytest installed and callable

8. **Environment Consistency:**
   ```bash
   which python
   # Expected: /path/to/mailreactor/.venv/bin/python
   ```
   - [ ] Python from venv (not system Python)

**Success Criteria:**
- [x] Automated verification script created
- [x] Integrated into setup workflow
- [x] Clear pass/fail feedback

---

### Subtask 1.4: Verify Manual Setup (Windows + Linux without Nix) (2h) ‚úÖ

**Deliverables:**
- [x] Verified working on Windows

**Platforms:**
- [x] Windows verified (HC confirmed working)

**Verification Steps (per platform):**

1. **Python Version:**
   ```bash
   python --version
   # Expected: Python 3.10.x or higher
   ```
   - [ ] Python 3.10+ installed

2. **Install uv:**
   ```bash
   pip install uv
   ```
   - [ ] uv installed successfully

3. **uv Package Manager:**
   ```bash
   uv --version
   # Expected: uv x.x.x
   ```
   - [ ] uv working

4. **Virtual Environment:**
   ```bash
   uv venv
   # Windows: .venv\Scripts\activate
   # Linux: source .venv/bin/activate
   ```
   - [ ] .venv created successfully
   - [ ] Virtual environment activated

5. **Dependency Installation:**
   ```bash
   uv pip install -e ".[dev]"
   # Should install FastAPI, pytest, etc.
   ```
   - [ ] All dependencies installed without errors

6. **Import Test:**
   ```bash
   python -c "import sys; print(sys.version)"
   # Expected: 3.10.x or higher
   ```
   - [ ] Python import works

7. **Pytest Available:**
   ```bash
   pytest --version
   # Expected: pytest x.x.x
   ```
   - [ ] pytest installed and callable

8. **Environment Consistency:**
   ```bash
   # Windows: where python
   # Linux: which python
   # Expected: /path/to/mailreactor/.venv/...
   ```
   - [ ] Python from venv (not system Python)

**Success Criteria:**
- [x] Windows setup verified and working
- [x] Same verification script works for all platforms

---

### Subtask 1.5: Fix Issues and Iterate (1h) ‚úÖ

**Issues Found & Fixed:**
1. **Python version mismatch:**
   - [x] Fixed: Added `--python python3.10` to manual setup
   - [x] Fixed: Added `--python $(which python)` to Nix setup

2. **Verification script not failing properly:**
   - [x] Fixed: Added proper exit codes and clear error messages
   - [x] Fixed: Shows actionable commands when failures occur


4. **Document Fixes:**
   - [x] Applied minimal configuration principle across all files
   - [x] Documented "add only when needed" in development-practices.md

**Success Criteria:**
- [x] All platform verification works
- [x] Documentation reflects actual minimal setup
- [x] No blocking issues remain

---

### Task #1 Gate Check ‚úÖ PASSED

**Before proceeding to Tasks #2-7, confirm:**

- [x] ‚úÖ Cross-platform verification script created (works on all platforms)
- [x] ‚úÖ HC confirms: "Windows setup works perfectly"
- [x] ‚úÖ Documentation complete and accurate (minimal approach)
- [x] ‚úÖ Both Nix and manual methods use Python 3.10
- [x] ‚úÖ Configuration principles documented

**Status:** ‚úÖ PASSED - Ready for remaining Sprint 0 tasks

---

## TDD Training Workshop (Optional but Recommended)

**Priority:** üü° **RECOMMENDED - After Task #1**  
**Estimated Effort:** 2-4 hours  
**Owner:** Tech Lead  
**Participants:** All developers

### Workshop Agenda

**Session 1: TDD Fundamentals (45 minutes)**
- [ ] Red-Green-Refactor cycle explained
- [ ] Why TDD is mandatory for Mail Reactor
- [ ] Benefits: Design feedback, regression prevention, living documentation

**Session 2: Pytest Basics (30 minutes)**
- [ ] Pytest discovery and execution
- [ ] Test structure: Arrange-Act-Assert
- [ ] Fixtures and parametrization
- [ ] Async testing with pytest-asyncio

**Session 3: Live Coding Example (60-90 minutes)**
- [ ] Choose example story (e.g., Story 2.1: Provider Detection)
- [ ] Write failing test first (RED)
- [ ] Implement minimal code to pass (GREEN)
- [ ] Refactor for clarity (REFACTOR)
- [ ] Repeat cycle 2-3 times

**Session 4: Mocking and Integration Tests (30 minutes)**
- [ ] Mock IMAP/SMTP servers
- [ ] FastAPI TestClient usage
- [ ] Integration test patterns

**Session 5: Q&A and Pre-commit Workflow (15 minutes)**
- [ ] Pre-commit hooks demo (coverage enforcement)
- [ ] Code review TDD checklist
- [ ] Common pitfalls and tips

**Deliverables:**
- [ ] Team confident in TDD workflow
- [ ] Example tests from workshop committed to `tests/examples/`
- [ ] TDD reference guide created (can be part of `docs/tdd-guide.md`)

---

## Task #2: TDD Infrastructure (2h) ‚è∏Ô∏è PENDING TEA INPUT

**Priority:** üü† **HIGH - Before Epic 1 implementation**  
**Estimated Effort:** 2 hours  
**Owner:** Tech Lead + TEA  
**Prerequisites:** Task #1 complete ‚úÖ  
**Status:** ‚è∏Ô∏è Awaiting Test Engineer design input before implementation

### Subtask 2.1: Create TDD Guide (1h)

**Deliverable:** `docs/tdd-guide.md`

**Content Requirements:**
- [ ] Red-Green-Refactor cycle explanation
- [ ] Mail Reactor TDD workflow (test-first for all stories)
- [ ] Pytest patterns and examples
- [ ] Mocking strategies for IMAP/SMTP
- [ ] Coverage requirements (80% minimum)

**Checklist:**
- [ ] `docs/tdd-guide.md` created
- [ ] Contains practical examples (code snippets)
- [ ] Links to pytest and pytest-asyncio docs

---

### Subtask 2.2: Create Test Templates (30 minutes)

**Deliverable:** `tests/templates/` directory

**Templates to Create:**
- [ ] `tests/templates/test_unit_template.py` (unit test example)
- [ ] `tests/templates/test_integration_template.py` (integration test with FastAPI TestClient)
- [ ] `tests/templates/test_e2e_template.py` (E2E test with real IMAP/SMTP)

**Example Unit Test Template:**
```python
"""
Unit test template for Mail Reactor

Copy this file to tests/unit/test_<feature>.py and customize.
"""
import pytest
from mailreactor.core import YourModule


class TestYourFeature:
    """Test suite for YourFeature."""

    def test_basic_functionality(self):
        """Test basic functionality works as expected."""
        # Arrange
        input_data = "example"
        
        # Act
        result = YourModule.process(input_data)
        
        # Assert
        assert result == "expected_output"

    @pytest.mark.parametrize("input,expected", [
        ("input1", "output1"),
        ("input2", "output2"),
    ])
    def test_parametrized(self, input, expected):
        """Test multiple scenarios with parametrization."""
        assert YourModule.process(input) == expected
```

**Checklist:**
- [ ] 3 test templates created
- [ ] Templates include comments explaining patterns
- [ ] Templates use pytest best practices

---

### Subtask 2.3: Configure Pre-commit Hooks (30 minutes)

**Deliverable:** `.pre-commit-config.yaml`

**Content:**
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
        additional_dependencies: [pydantic>=2.0]

  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.4.0
    hooks:
      - id: detect-secrets
        args: ['--baseline', '.secrets.baseline']

  - repo: local
    hooks:
      - id: pytest-coverage
        name: pytest with coverage
        entry: pytest
        args: [--cov=src, --cov-fail-under=80]
        language: system
        pass_filenames: false
        always_run: true
```

**Setup Steps:**
1. **Install pre-commit:**
   ```bash
   uv pip install pre-commit
   pre-commit install
   ```
   - [ ] pre-commit installed

2. **Create secrets baseline:**
   ```bash
   detect-secrets scan > .secrets.baseline
   ```
   - [ ] `.secrets.baseline` created

3. **Test pre-commit:**
   ```bash
   pre-commit run --all-files
   ```
   - [ ] All hooks pass

**Checklist:**
- [ ] `.pre-commit-config.yaml` created
- [ ] Pre-commit installed in all developer environments
- [ ] Hooks enforce: linting, type checking, secret detection, coverage

---

### Subtask 2.4: Create Code Review Checklist (optional, 15 minutes)

**Deliverable:** `.github/PULL_REQUEST_TEMPLATE.md`

**Content:**
```markdown
## Description
<!-- Brief description of changes -->

## Related Story
<!-- Link to story (e.g., Story 1.1: Project Structure) -->

## TDD Checklist
- [ ] Tests written BEFORE implementation (test-first)
- [ ] All tests pass (`pytest`)
- [ ] Coverage ‚â•80% (`pytest --cov=src`)
- [ ] No secrets detected (`detect-secrets scan`)

## Code Quality
- [ ] Ruff linting passes (`ruff check .`)
- [ ] Type checking passes (`mypy src/`)
- [ ] Code formatted (`ruff format .`)

## Testing
- [ ] Unit tests added/updated
- [ ] Integration tests added/updated (if applicable)
- [ ] E2E tests added/updated (if applicable)

## Documentation
- [ ] Docstrings added for public functions/classes
- [ ] README updated (if user-facing changes)
- [ ] Architecture notes updated (if architectural changes)
```

**Checklist:**
- [ ] Pull request template created
- [ ] Template enforces TDD workflow
- [ ] Template includes all quality gates

---

## Task #3: Mock IMAP/SMTP Server Setup (4h)

**Priority:** üü† **HIGH - Required for Epic 2-4 integration tests**  
**Estimated Effort:** 4 hours  
**Owner:** Tech Lead  
**Prerequisites:** Task #1 complete

### Subtask 3.1: Install and Configure Greenmail (2h)

**Deliverable:** `tests/docker-compose.test.yml`

**Content:**
```yaml
version: '3.8'

services:
  greenmail:
    image: greenmail/standalone:2.0.0
    ports:
      - "3143:3143"  # IMAP
      - "3993:3993"  # IMAPS
      - "3025:3025"  # SMTP
      - "3465:3465"  # SMTPS
    environment:
      GREENMAIL_OPTS: >
        -Dgreenmail.setup.test.all
        -Dgreenmail.hostname=0.0.0.0
        -Dgreenmail.auth.disabled=false
        -Dgreenmail.verbose
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "3143"]
      interval: 5s
      timeout: 3s
      retries: 3
```

**Setup Steps:**
1. **Create docker-compose file:**
   - [ ] `tests/docker-compose.test.yml` created

2. **Start mock server:**
   ```bash
   cd tests
   docker-compose -f docker-compose.test.yml up -d
   ```
   - [ ] Greenmail container starts successfully

3. **Test IMAP connection:**
   ```bash
   python -c "from imapclient import IMAPClient; c = IMAPClient('localhost', 3143, ssl=False); c.login('test@localhost', 'test'); print('IMAP OK')"
   ```
   - [ ] IMAP connection succeeds

4. **Test SMTP connection:**
   ```bash
   python -c "import smtplib; s = smtplib.SMTP('localhost', 3025); s.quit(); print('SMTP OK')"
   ```
   - [ ] SMTP connection succeeds

**Checklist:**
- [ ] Greenmail configured and running
- [ ] IMAP port 3143 accessible
- [ ] SMTP port 3025 accessible
- [ ] Health check passes

---

### Subtask 3.2: Document Mock Server API (1h)

**Deliverable:** `tests/README.md`

**Content Requirements:**
- [ ] How to start/stop mock servers
- [ ] Mock server ports and protocols
- [ ] Test account credentials
- [ ] Mock server reset/cleanup
- [ ] Example usage in tests

**Example Section:**
```markdown
# Testing with Mock IMAP/SMTP Servers

## Starting Mock Servers

```bash
cd tests
docker-compose -f docker-compose.test.yml up -d
```

## Test Accounts

Greenmail auto-creates accounts on first login:
- **Email:** any@localhost (e.g., test@localhost)
- **Password:** any password (e.g., "test")
- **IMAP:** localhost:3143 (no SSL)
- **SMTP:** localhost:3025 (no SSL)

## Example Integration Test

```python
import pytest
from imapclient import IMAPClient

@pytest.fixture
def mock_imap():
    """IMAP client connected to mock server."""
    client = IMAPClient('localhost', 3143, ssl=False)
    client.login('test@localhost', 'test')
    yield client
    client.logout()

def test_imap_search(mock_imap):
    """Test IMAP search with mock server."""
    messages = mock_imap.search(['ALL'])
    assert isinstance(messages, list)
```

## Cleanup

```bash
docker-compose -f docker-compose.test.yml down -v
```
```

**Checklist:**
- [ ] `tests/README.md` created
- [ ] Startup/shutdown instructions clear
- [ ] Example tests provided
- [ ] Troubleshooting section included

---

### Subtask 3.3: Create Mock Server Fixtures (1h)

**Deliverable:** `tests/conftest.py` with reusable fixtures

**Content:**
```python
"""
Shared pytest fixtures for Mail Reactor tests.
"""
import pytest
from imapclient import IMAPClient
import smtplib
from httpx import AsyncClient
from mailreactor.main import app


@pytest.fixture(scope="session")
def mock_imap_host():
    """Mock IMAP server host and port."""
    return "localhost", 3143


@pytest.fixture(scope="session")
def mock_smtp_host():
    """Mock SMTP server host and port."""
    return "localhost", 3025


@pytest.fixture
def mock_imap(mock_imap_host):
    """IMAP client connected to mock server."""
    host, port = mock_imap_host
    client = IMAPClient(host, port, ssl=False)
    client.login("test@localhost", "test")
    yield client
    client.logout()


@pytest.fixture
def mock_smtp(mock_smtp_host):
    """SMTP connection to mock server."""
    host, port = mock_smtp_host
    smtp = smtplib.SMTP(host, port)
    yield smtp
    smtp.quit()


@pytest.fixture
async def api_client():
    """FastAPI test client."""
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client
```

**Checklist:**
- [ ] `tests/conftest.py` created
- [ ] IMAP fixture provided
- [ ] SMTP fixture provided
- [ ] FastAPI test client fixture provided
- [ ] Fixtures tested and working

---

## Task #4: Test Project Structure (2h)

**Priority:** üü† **HIGH - Required for organizing tests**  
**Estimated Effort:** 2 hours  
**Owner:** Tech Lead  
**Prerequisites:** Task #1 complete

### Subtask 4.1: Create Test Directory Structure (30 minutes)

**Deliverables:**
```
tests/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ conftest.py (from Task #3)
‚îú‚îÄ‚îÄ docker-compose.test.yml (from Task #3)
‚îú‚îÄ‚îÄ README.md (from Task #3)
‚îú‚îÄ‚îÄ templates/ (from Task #2)
‚îÇ   ‚îú‚îÄ‚îÄ test_unit_template.py
‚îÇ   ‚îú‚îÄ‚îÄ test_integration_template.py
‚îÇ   ‚îî‚îÄ‚îÄ test_e2e_template.py
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ e2e/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ security/
    ‚îî‚îÄ‚îÄ __init__.py
```

**Commands:**
```bash
mkdir -p tests/{unit,integration,e2e,performance,security}
touch tests/{unit,integration,e2e,performance,security}/__init__.py
```

**Checklist:**
- [ ] All directories created
- [ ] `__init__.py` in each test category
- [ ] Structure matches Test Design recommendations

---

### Subtask 4.2: Document Testing Philosophy (1h)

**Deliverable:** Update `tests/README.md` with testing philosophy

**Add Section:**
```markdown
# Testing Philosophy

## Test Distribution (from Test Design)

- **Unit Tests (50%):** Business logic, Pydantic models, utilities
- **Integration Tests (35%):** API endpoints with mocked IMAP/SMTP
- **E2E Tests (15%):** Critical paths with real servers

## Test Organization

### `tests/unit/`
- Pure Python logic tests
- Fast (<1ms per test)
- No network, no file I/O
- Mock all external dependencies

**Example:** Provider detection logic, email parsing, configuration loading

### `tests/integration/`
- API endpoint tests with FastAPI TestClient
- Mock IMAP/SMTP servers (localhost)
- Moderate speed (~100ms per test)
- Focus on API contracts and error handling

**Example:** POST /accounts with mock IMAP, GET /messages with mock search

### `tests/e2e/`
- Critical user journeys
- Real IMAP/SMTP servers (Greenmail or test accounts)
- Slow (~5s per test)
- Happy paths and critical risks only

**Example:** Add Gmail account ‚Üí send email ‚Üí retrieve email

### `tests/performance/`
- Startup time benchmarks (NFR-P1: <3 seconds)
- API latency benchmarks (NFR-P2: <200ms p95)
- Memory footprint tests (NFR-P4: <100MB)

### `tests/security/`
- Credential leak detection (scan logs for passwords)
- API key security tests
- Dependency vulnerability checks

## Coverage Requirements

- **Minimum:** 80% (NFR-M1)
- **Target:** 85%+
- **Enforced:** Pre-commit hook fails if <80%

## Running Tests

**All tests:**
```bash
pytest
```

**By category:**
```bash
pytest tests/unit/          # Fast feedback
pytest tests/integration/   # API validation
pytest tests/e2e/           # Full stack
```

**With coverage:**
```bash
pytest --cov=src --cov-report=html
open htmlcov/index.html
```
```

**Checklist:**
- [ ] Testing philosophy documented
- [ ] Test categories explained with examples
- [ ] Coverage requirements stated
- [ ] Command reference provided

---

### Subtask 4.3: Create Example Tests (30 minutes)

**Deliverable:** Example test in each category

**`tests/unit/test_example.py`:**
```python
"""Example unit test."""
def test_example():
    """Example test that always passes."""
    assert 1 + 1 == 2
```

**`tests/integration/test_example.py`:**
```python
"""Example integration test."""
import pytest


@pytest.mark.asyncio
async def test_api_example(api_client):
    """Example API test (health endpoint will exist after Story 1.5)."""
    # This will fail until Story 1.5 implemented
    # response = await api_client.get("/health")
    # assert response.status_code == 200
    assert True  # Placeholder
```

**`tests/e2e/test_example.py`:**
```python
"""Example E2E test."""
import pytest


@pytest.mark.e2e
def test_e2e_example():
    """Example E2E test placeholder."""
    # Will implement real E2E tests after Epic 2
    assert True
```

**Run Tests:**
```bash
pytest tests/
# Expected: 3 tests pass
```

**Checklist:**
- [ ] Example tests in each category
- [ ] All example tests pass
- [ ] pytest discovery works (`pytest` finds all tests)

---

## Task #5: Security Scanning Setup (2h)

**Priority:** üü† **HIGH - Required for Epic 5 (Security)**  
**Estimated Effort:** 2 hours  
**Owner:** Tech Lead  
**Prerequisites:** Task #1, Task #2 (pre-commit config) complete

### Subtask 5.1: Configure detect-secrets (30 minutes)

**Already done in Task #2.3**, verify:
- [ ] `detect-secrets` in `.pre-commit-config.yaml`
- [ ] `.secrets.baseline` file created
- [ ] Pre-commit hook blocks commits with secrets

**Test:**
```bash
# Create test file with fake secret
echo "password = 'my-secret-password-123'" > test_secret.py

# Try to commit (should fail)
git add test_secret.py
git commit -m "test"
# Expected: detect-secrets hook fails

# Clean up
rm test_secret.py
git reset HEAD test_secret.py
```

**Checklist:**
- [ ] detect-secrets blocks commits with secrets
- [ ] False positives added to baseline
- [ ] Team trained on updating baseline

---

### Subtask 5.2: Create Log Scanning Integration Test (1h)

**Deliverable:** `tests/security/test_credential_leaks.py`

**Content:**
```python
"""
Security tests for credential leak detection.

Validates NFR-S1: Credentials never logged or exposed.
"""
import pytest
import re
from io import StringIO
import logging


class TestCredentialLeaks:
    """Test that credentials never appear in logs."""

    CREDENTIAL_PATTERNS = [
        r'password\s*[:=]\s*["\']([^"\']+)["\']',
        r'api[_-]?key\s*[:=]\s*["\']([^"\']+)["\']',
        r'secret\s*[:=]\s*["\']([^"\']+)["\']',
        r'token\s*[:=]\s*["\']([^"\']+)["\']',
    ]

    def test_credentials_not_in_logs(self, caplog):
        """Verify credentials never appear in logs."""
        # This will be implemented after Story 2.6 (StateManager)
        # For now, test the pattern matching works
        
        test_log = "INFO: Account connected email=test@example.com"
        
        for pattern in self.CREDENTIAL_PATTERNS:
            matches = re.findall(pattern, test_log, re.IGNORECASE)
            assert len(matches) == 0, f"Found potential credential in log: {matches}"

    @pytest.mark.asyncio
    async def test_api_error_no_password_leak(self, api_client):
        """Verify API errors don't leak passwords."""
        # Will implement after Story 2.4 (Connection Validation)
        # For now, placeholder
        assert True
```

**Checklist:**
- [ ] Log scanning test created
- [ ] Credential patterns defined
- [ ] Test passes (placeholder for now)

---

### Subtask 5.3: Configure Safety Check in CI (30 minutes)

**Deliverable:** Add `safety` check to GitHub Actions workflow

**Will be completed in Task #7 (CI/CD Pipeline)**

**For now, test locally:**
```bash
uv pip install safety
safety check
```

**Expected:** Report of known vulnerabilities in dependencies

**Checklist:**
- [ ] Safety installed locally
- [ ] Safety check runs without blocking errors
- [ ] Note to add to CI in Task #7

---

## Task #6: Performance Benchmark Infrastructure (3h)

**Priority:** üü† **HIGH - Required for NFR-P1, NFR-P2 validation**  
**Estimated Effort:** 3 hours  
**Owner:** Tech Lead  
**Prerequisites:** Task #1 complete

### Subtask 6.1: Configure pytest-benchmark (1h)

**Install:**
```bash
uv pip install pytest-benchmark
```

**Create:** `tests/performance/test_startup.py`

**Content:**
```python
"""
Performance tests for Mail Reactor.

Validates:
- NFR-P1: Startup time <3 seconds
- NFR-P2: API response <200ms p95
"""
import pytest
import time
import subprocess


class TestStartupPerformance:
    """Test startup time meets NFR-P1 (<3 seconds)."""

    @pytest.mark.benchmark
    def test_startup_time(self, benchmark):
        """Measure cold start time from import to operational."""
        def startup():
            # Will implement after Story 1.4 (CLI Start)
            # For now, measure import time
            start = time.perf_counter()
            import mailreactor
            return time.perf_counter() - start

        result = benchmark(startup)
        
        # NFR-P1: <3 seconds (lenient for now, will tighten after Story 1.4)
        assert benchmark.stats.median < 3.0, f"Startup too slow: {benchmark.stats.median}s"


class TestAPILatency:
    """Test API response latency meets NFR-P2 (<200ms p95)."""

    @pytest.mark.asyncio
    @pytest.mark.benchmark
    async def test_health_endpoint_latency(self, api_client, benchmark):
        """Health check responds within 50ms p95."""
        # Will implement after Story 1.5 (Health Endpoint)
        # For now, placeholder
        async def health_check():
            # response = await api_client.get("/health")
            # return response.elapsed.total_seconds() * 1000  # ms
            return 10  # Placeholder: 10ms

        result = benchmark(health_check)
        
        # NFR-P2: Health check <50ms p95
        assert benchmark.stats.stats.q_95 < 50, f"Latency too high: {benchmark.stats.stats.q_95}ms"
```

**Run Benchmarks:**
```bash
pytest tests/performance/ --benchmark-only
```

**Checklist:**
- [ ] pytest-benchmark installed
- [ ] Startup time benchmark created
- [ ] API latency benchmark created
- [ ] Benchmarks run successfully (placeholders for now)

---

### Subtask 6.2: Create GitHub Actions Benchmark Workflow (1h)

**Deliverable:** `.github/workflows/benchmark.yml`

**Content:**
```yaml
name: Performance Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: nixos/nix-action@v23
        with:
          install_url: https://releases.nixos.org/nix/nix-2.18.1/install
          
      - name: Setup environment
        run: |
          nix develop --command bash -c "
            uv venv
            source .venv/bin/activate
            uv pip install -e '.[dev]'
          "
      
      - name: Run benchmarks
        run: |
          nix develop --command bash -c "
            source .venv/bin/activate
            pytest tests/performance/ --benchmark-only --benchmark-json=benchmark.json
          "
      
      - name: Check thresholds
        run: |
          # Will implement threshold checks after baseline established
          echo "Benchmark results in benchmark.json"
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark.json
```

**Checklist:**
- [ ] Benchmark workflow created
- [ ] Runs on push to main/develop
- [ ] Uploads benchmark results as artifacts
- [ ] Note to add threshold checks after baseline established

---

### Subtask 6.3: Document Performance Testing (1h)

**Deliverable:** Add to `tests/README.md`

**Content:**
```markdown
# Performance Testing

## Running Benchmarks Locally

```bash
pytest tests/performance/ --benchmark-only
```

## Benchmark Results

Results include:
- **Mean:** Average execution time
- **Median:** Middle value (more robust than mean)
- **StdDev:** Standard deviation (consistency measure)
- **Min/Max:** Fastest and slowest runs
- **q_95:** 95th percentile (NFR requirement)

## NFR Thresholds

- **NFR-P1 (Startup):** <3.0 seconds median
- **NFR-P2 (Health Check):** <50ms p95
- **NFR-P2 (API Endpoints):** <200ms p95

## Establishing Baselines

After Story 1.4 (CLI Start) implementation:
1. Run benchmarks 10 times
2. Record median startup time
3. Set CI threshold = baseline + 1 second
4. Document baseline in `docs/performance-baseline.md`

## CI Integration

Benchmarks run on every push to main/develop.
Results uploaded as GitHub Actions artifacts.

## Troubleshooting Slow Benchmarks

If benchmarks fail thresholds:
1. Check for recent changes (git log)
2. Profile with cProfile: `python -m cProfile -o profile.stats script.py`
3. Analyze with snakeviz: `pip install snakeviz && snakeviz profile.stats`
4. Optimize hotspots
```

**Checklist:**
- [ ] Performance testing documented
- [ ] NFR thresholds listed
- [ ] Baseline establishment process explained
- [ ] Troubleshooting guide provided

---

## Task #7: CI/CD Pipeline Configuration (3h)

**Priority:** üü† **HIGH - Required before Sprint 1**  
**Estimated Effort:** 3 hours  
**Owner:** Tech Lead  
**Prerequisites:** Tasks #1-6 complete

### Subtask 7.1: Create Main CI Workflow (1.5h)

**Deliverable:** `.github/workflows/ci.yml`

**Content:**
```yaml
name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Nix
        uses: cachix/install-nix-action@v23
        with:
          nix_path: nixpkgs=channel:nixos-unstable
      
      - name: Setup Nix environment
        run: |
          nix develop --command bash -c "
            python --version
            uv --version
          "
      
      - name: Install dependencies
        run: |
          nix develop --command bash -c "
            uv venv
            source .venv/bin/activate
            uv pip install -e '.[dev]'
          "
      
      - name: Start mock servers
        run: |
          cd tests
          docker-compose -f docker-compose.test.yml up -d
          sleep 5  # Wait for servers to be ready
      
      - name: Run linting
        run: |
          nix develop --command bash -c "
            source .venv/bin/activate
            ruff check .
            ruff format --check .
          "
      
      - name: Run type checking
        run: |
          nix develop --command bash -c "
            source .venv/bin/activate
            mypy src/
          "
      
      - name: Run tests with coverage
        run: |
          nix develop --command bash -c "
            source .venv/bin/activate
            pytest tests/ --cov=src --cov-report=xml --cov-report=term
          "
      
      - name: Check coverage threshold
        run: |
          nix develop --command bash -c "
            source .venv/bin/activate
            pytest --cov=src --cov-fail-under=80
          "
      
      - name: Run security checks
        run: |
          nix develop --command bash -c "
            source .venv/bin/activate
            safety check
            detect-secrets scan
          "
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          fail_ci_if_error: true
      
      - name: Stop mock servers
        if: always()
        run: |
          cd tests
          docker-compose -f docker-compose.test.yml down -v
```

**Checklist:**
- [ ] CI workflow created
- [ ] Runs on push and pull requests
- [ ] Tests Python 3.10, 3.11, 3.12
- [ ] Includes linting, type checking, tests, security checks
- [ ] Uploads coverage to Codecov

---

### Subtask 7.2: Configure Branch Protection (30 minutes)

**Manual Steps (in GitHub repository settings):**

1. **Navigate to:** Settings ‚Üí Branches ‚Üí Branch protection rules
2. **Add rule for:** `main` branch
3. **Configure:**
   - [ ] Require pull request reviews before merging (1 approval)
   - [ ] Require status checks to pass before merging
     - [ ] `test (3.10)`
     - [ ] `test (3.11)`
     - [ ] `test (3.12)`
   - [ ] Require branches to be up to date before merging
   - [ ] Require conversation resolution before merging
   - [ ] Include administrators (enforce for everyone)

**Checklist:**
- [ ] Branch protection enabled for `main`
- [ ] CI checks required before merge
- [ ] Pull request reviews required

---

### Subtask 7.3: Document CI/CD Pipeline (1h)

**Deliverable:** Add to `docs/development-practices.md`

**Content:**
```markdown
# CI/CD Pipeline

## Overview

Mail Reactor uses GitHub Actions for continuous integration and deployment.

## CI Workflow

**Triggers:**
- Push to `main` or `develop` branches
- Pull requests to `main`

**Matrix Testing:**
- Python 3.10, 3.11, 3.12
- Ubuntu latest

**Pipeline Stages:**
1. **Setup:** Install Nix, create venv, install dependencies
2. **Start Mock Servers:** Docker Compose with Greenmail
3. **Linting:** Ruff check + format validation
4. **Type Checking:** mypy strict mode
5. **Tests:** pytest with coverage reporting
6. **Coverage Check:** Fail if <80% coverage
7. **Security:** safety check + detect-secrets
8. **Upload Coverage:** Codecov integration
9. **Cleanup:** Stop mock servers

## Local Development Workflow

### Before Committing

Pre-commit hooks automatically run:
- Ruff linting and formatting
- mypy type checking
- detect-secrets scanning
- pytest with 80% coverage requirement

**If hooks fail:** Fix issues before commit is allowed.

### Pull Request Workflow

1. Create feature branch: `git checkout -b feature/story-1.1`
2. Implement with TDD (tests first)
3. Run tests locally: `pytest`
4. Commit (pre-commit hooks run automatically)
5. Push to GitHub: `git push origin feature/story-1.1`
6. Create pull request
7. CI runs (must pass)
8. Request review (1 approval required)
9. Merge to main

## Troubleshooting CI Failures

### Linting Failures
```bash
ruff check . --fix  # Auto-fix issues
ruff format .       # Format code
```

### Type Checking Failures
```bash
mypy src/  # Run locally to see errors
```

### Test Failures
```bash
pytest -v  # Verbose output
pytest tests/unit/test_specific.py::test_name  # Run specific test
```

### Coverage Failures
```bash
pytest --cov=src --cov-report=html
open htmlcov/index.html  # View coverage report
# Add tests for uncovered code
```

### Security Failures
```bash
safety check  # Check for vulnerable dependencies
detect-secrets scan  # Scan for secrets
```
```

**Checklist:**
- [ ] CI/CD pipeline documented
- [ ] Local workflow explained
- [ ] Pull request process defined
- [ ] Troubleshooting guide provided

---

## Sprint 0 Completion Checklist

### Task Completion

- [ ] **Task #1:** Environment Setup & Verification ‚úÖ
  - [ ] Subtask 1.1: Configurations created
  - [ ] Subtask 1.2: Documentation written
  - [ ] Subtask 1.3: Nix setup verified (macOS, Linux/WSL2)
  - [ ] Subtask 1.4: Manual setup verified (Windows, Linux)
  - [ ] Subtask 1.5: Issues fixed, all platforms pass

- [ ] **TDD Training:** Workshop completed (optional but recommended)

- [ ] **Task #2:** TDD Infrastructure ‚úÖ
  - [ ] Subtask 2.1: TDD guide created
  - [ ] Subtask 2.2: Test templates created
  - [ ] Subtask 2.3: Pre-commit hooks configured
  - [ ] Subtask 2.4: PR template created

- [ ] **Task #3:** Mock IMAP/SMTP Server ‚úÖ
  - [ ] Subtask 3.1: Greenmail installed and configured
  - [ ] Subtask 3.2: Mock server API documented
  - [ ] Subtask 3.3: Fixtures created in conftest.py

- [ ] **Task #4:** Test Project Structure ‚úÖ
  - [ ] Subtask 4.1: Directory structure created
  - [ ] Subtask 4.2: Testing philosophy documented
  - [ ] Subtask 4.3: Example tests created

- [ ] **Task #5:** Security Scanning ‚úÖ
  - [ ] Subtask 5.1: detect-secrets configured
  - [ ] Subtask 5.2: Log scanning test created
  - [ ] Subtask 5.3: Safety check configured

- [ ] **Task #6:** Performance Benchmarks ‚úÖ
  - [ ] Subtask 6.1: pytest-benchmark configured
  - [ ] Subtask 6.2: Benchmark workflow created
  - [ ] Subtask 6.3: Performance testing documented

- [ ] **Task #7:** CI/CD Pipeline ‚úÖ
  - [ ] Subtask 7.1: Main CI workflow created
  - [ ] Subtask 7.2: Branch protection configured
  - [ ] Subtask 7.3: CI/CD documented

### Verification

- [ ] All platform environments verified and working
- [ ] Pre-commit hooks block bad commits
- [ ] Mock servers start/stop successfully
- [ ] Example tests pass
- [ ] CI pipeline runs and passes
- [ ] Branch protection enforces quality gates

### Epic 6 Decision

- [ ] **Decision Made:** Defer to Phase 2 ‚úÖ OR Include in MVP ‚ö†Ô∏è
- [ ] **Documented in:** Workflow status updated

### Documentation

- [ ] `docs/environment-setup-guide.md` complete
- [ ] `docs/tdd-guide.md` complete
- [ ] `tests/README.md` complete
- [ ] `docs/development-practices.md` updated with CI/CD section

### Team Readiness

- [ ] HC confirms: "Windows setup works perfectly"
- [ ] Team confirms: "macOS and Linux/WSL2 setups work perfectly"
- [ ] All developers have pre-commit hooks enabled
- [ ] TDD training completed (or scheduled for Sprint 1)

---

## ‚úÖ Sprint 0 Complete - Ready for Sprint Planning

**When all checklists pass:**
1. Update `docs/bmm-workflow-status.yaml` ‚Üí Mark Sprint 0 complete
2. Invoke Scrum Master: `/bmad:bmm:workflows:sprint-planning`
3. Begin Epic 1 implementation!

---

**Generated:** 2025-11-26  
**For:** Mail Reactor Sprint 0  
**Reference:** Implementation Readiness Report (docs/implementation-readiness-report-20251126.md)
